{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2: Discovery of Frequent Itemsets and Association Rules\n",
    "\n",
    "Date: 22/11/2021\n",
    "\n",
    "Authors: Alessandro Sanvito and Thuany Karoline Stuart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution\n",
    "\n",
    "The code in the src/ folder reproduces the algorithm described in \"R. Agrawal and R. Srikant.\n",
    " [Fast Algorithms for Mining Association Rules (Links to an external site.)](http://www.vldb.org/conf/1994/P487.PDF),\n",
    " VLDB '94\".\n",
    "\n",
    "The code leverages Python's higher-order functions map() and filter() for fast iteration and Itertools for generating\n",
    "the itemsets to achieve the best performance.\n",
    "\n",
    "An element of particular interest in the implementation is the way the support is counted.\n",
    "At first, the a priori algorithm implemented the count of the candidate frequent itemsets in every transaction\n",
    "by iterating in a double loop. This approach proved to be inefficient and led to unmanageable execution time.\n",
    "The current solution, on the other hand, considers once each basket. For each basket, it generates all the itemsets\n",
    "of the considered length. Then, for each item set, it checks in O(1) if it is a candidate and in case it proceeds to\n",
    "increase the support.\n",
    "\n",
    "Overall, the implementation achieves considerable performance on the large given dataset, in the spirit of the original algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Run\n",
    "\n",
    "To run the conducted experiments, follow the steps:\n",
    "\n",
    "1. Unzip the file containing the homework.\n",
    "1. Ensure to have Python 3 installed on your machine.\n",
    "1. Ensure that NumPy, Typing, Itertools and Jupyter Notebook are installed in your environment.\n",
    "1. Download the \"T10I4D100K.dat\" dataset file from the Homework 2 description in Canvas.\n",
    "1. Move the dataset file to the folder ./data\n",
    "1. Start Jupyter Notebook.\n",
    "1. In Jupyter Notebook, open the notebook \"Discovery of Frequent Itemsets and Association Rules\" in the folder /src of the homework.\n",
    "1. Press \"run all\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "\n",
    "from a_priori import *\n",
    "from rule_generation import *\n",
    "\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/c/users/aless/desktop/ID2222-Data-Mining-Sanvito-Stuart/lab2/data/T10I4D100K.dat'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = os.path.dirname(os.getcwd())\n",
    "data_path = os.path.join(path, 'data', 'T10I4D100K.dat')\n",
    "data_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequent Itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The market contains 870 different items.\n",
      "The average support is 1161.18\n",
      "The most frequent singletons have been calculated. 375 singletons was/were found.\n",
      "Computing frequent itemsets of length 2...\n",
      "70125 candidates generated!\n",
      "Done! 9 frequent items was/were found.\n",
      "Computing frequent itemsets of length 3...\n",
      "4 candidates generated!\n",
      "Done! 1 frequent items was/were found.\n",
      "\n",
      "In total 385 frequent items were found.\n"
     ]
    }
   ],
   "source": [
    "freq_items = find_frequent_item_sets(file=data_path, s=1000, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supports = [500, 750, 1000, 1250, 1500]\n",
    "durations = []\n",
    "n_freq_items = []\n",
    "\n",
    "for s in supports:\n",
    "  start = time.time()\n",
    "  freq_items = find_frequent_item_sets(file=data_path, s=s, verbose=False)\n",
    "  duration = time.time() - start\n",
    "  \n",
    "  durations.append(duration)\n",
    "  n_freq_items.append(len(freq_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(supports, durations)\n",
    "plt.xlabel('Support threshold')\n",
    "plt.ylabel('Execution time (s)')\n",
    "plt.title('Execution time vs. Support threshold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(supports, n_freq_items)\n",
    "plt.xlabel('Support threshold')\n",
    "plt.ylabel('Number of frequent items')\n",
    "plt.title('Number of frequent items vs. Support threshold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_items = find_frequent_item_sets(file=data_path, s=1000, verbose=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidences = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "durations = []\n",
    "n_rules = []\n",
    "\n",
    "for c in confidences:\n",
    "  start = time.time()\n",
    "  rules = generate_rules(frequent_item_sets=freq_items, c=c)\n",
    "  duration = time.time() - start\n",
    "  \n",
    "  durations.append(duration)\n",
    "  n_rules.append(len(rules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(confidences, durations)\n",
    "plt.xlabel('Confidence threshold')\n",
    "plt.ylabel('Execution time (s)')\n",
    "plt.title('Execution time vs. Confidence threshold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(confidences, n_rules)\n",
    "plt.xlabel('Confidence threshold')\n",
    "plt.ylabel('Number of rules')\n",
    "plt.title('Number of rules vs. Confidence threshold')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}